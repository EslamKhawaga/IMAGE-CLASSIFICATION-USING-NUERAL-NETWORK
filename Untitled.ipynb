{"metadata":{"orig_nbformat":4,"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# Downloading Packages\n\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nfrom testCases import *\nfrom dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\nfrom public_tests import *\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom dnn_app_utils_v3 import *\n\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\n# Loading and exploring the data set \n\ntrain_x_orig, train_y, test_x_orig, test_y, classes = load_data()\nm_train = train_x_orig.shape[0]\nnum_px = train_x_orig.shape[1]\nm_test = test_x_orig.shape[0]\n\n\n# Reshape the training and test examples \n\ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T  \ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n\nprint (\"train_x's shape: \" + str(train_x.shape))\nprint (\"test_x's shape: \" + str(test_x.shape))\n\n### CONSTANTS DEFINING THE MODEL ####\nn_x = 12288     # num_px * num_px * 3\nn_h = 7\nn_y = 1\nlayers_dims = (n_x, n_h, n_y)\nlearning_rate = 0.0075\n\n# Initializing the parameters for a 2-layer network\n\ndef initialize_parameters(n_x, n_h, n_y):\n  \n    W1 = np.random.randn(n_h,n_x)*0.01\n    b1 = np.zeros(shape=(n_h,1))\n    W2 = np.random.randn(n_y, n_h)* 0.01\n    b2 = np.zeros(shape=(n_y,1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters    \n\n\n# Building a L-layer Neural Network\n\ndef initialize_parameters_deep(layer_dims):\n   \n    parameters = {}\n    L = len(layer_dims) # number of layers in the network\n\n    for l in range(1, L):\n        \n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n            \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\n\n# Building the linear part of a forward propagation\n\ndef linear_forward(A, W, b):\n    \n    Z = np.dot(W,A)+b\n    cache = (A, W, b)\n    \n    return Z, cache\n\n#  linear_activation_forward\n\ndef linear_activation_forward(A_prev, W, b, activation):\n      \n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n        \n    elif activation == \"relu\":\n    \n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    \n    cache = (linear_cache, activation_cache)\n\n    return A, cache\n\n# L_model_forward\n\ndef L_model_forward(X, parameters):\n \n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    \n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)],  activation='relu')\n        caches.append(cache)\n        AL, cache = linear_activation_forward(A,parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid')\n        caches.append(cache)    \n              \n    return AL, caches\n\n# Calculating the cost\n\ndef compute_cost(AL, Y):\n    \n    m = Y.shape[1]\n    cost = -1/m * np.sum(np.multiply(Y,np.log(AL)) + np.multiply((1-Y),(np.log(1-AL)))\n    cost = np.squeeze(cost)\n    \n    return cost\n\n# Building the linear part of the backword propagation\n\ndef linear_backward(dZ, cache):\n   \n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = 1/m * np.dot(dZ,cache[0].T)\n    db = 1/m * np.sum(dZ,axis=1, keepdims= True)\n    dA_prev = np.dot(cache[1].T,dZ)\n        \n    return dA_prev, dW, db\n\n# Calculating the Activation function: linear_activation_backward\n\ndef linear_activation_backward(dA, cache, activation):\n   \n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        \n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n        return dA_prev, dW, db\n                         \n# L_model_backward\n\ndef L_model_backward(AL, Y, caches):\n    \n    grads = {}\n    L = len(caches) \n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) \n    \n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    \n    current_cache = caches[L-1]\n    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache , activation = \"sigmoid\")\n    grads[\"dA\" + str(L-1)] = dA_prev_temp \n    grads[\"dW\" + str(L)] = dW_temp\n    grads[\"db\" + str(L)] = db_temp\n   \n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n        \n\n    return grads        \n                         \n# Updating the Parameters\n                      \ndef update_parameters(params, grads, learning_rate):\n   \n    parameters = params.copy()\n    L = len(parameters) // 2 # number of layers in the neural network\n\n    for l in range(L):     \n                         \n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n        \n    return parameters\n                         \n                         \n                                                  ### Getting the Two-layer Neural Network  ###\n                         \n\ndef two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n\n    np.random.seed(1)\n    grads = {}\n    costs = []                              # to keep track of the cost\n    m = X.shape[1]                           # number of examples\n    (n_x, n_h, n_y) = layers_dims\n    \n    # Initialize parameters dictionary\n\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n\n        A1, cache1 = linear_activation_forward(X, W1, b1, activation= \"relu\")\n        A2, cache2 = linear_activation_forward(A1, W2, b2, activation= \"sigmoid\")\n                         \n        # Compute cost\n        cost = compute_cost(A2, Y)\n            \n        # Initializing backward propagation\n        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = \"sigmoid\")\n        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = \"relu\")\n\n        grads['dW1'] = dW1\n        grads['db1'] = db1\n        grads['dW2'] = dW2\n        grads['db2'] = db2\n        parameters = update_parameters(parameters, grads, learning_rate)\n        W1 = parameters[\"W1\"]\n        b1 = parameters[\"b1\"]\n        W2 = parameters[\"W2\"]\n        b2 = parameters[\"b2\"]\n        \n        # Print the cost every 100 iterations\n        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n\n    return parameters, costs\n\ndef plot_costs(costs, learning_rate=0.0075):\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n                         \n # Training the model\n         parameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\n         plot_costs(costs, learning_rate)\n                         \n                                          ### Getting the Two-layer Neural Network  ###\n\ndef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n \n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    \n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims)\n    \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        AL, caches = L_model_forward(X, parameters)\n        \n        # Compute cost.\n        cost = compute_cost(AL, Y)\n       \n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n      \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n    \n        # Print the cost every 100 iterations\n        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n    \n    return parameters, costs\n# Training the model                      \n    parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)","metadata":{},"execution_count":null,"outputs":[],"id":"6eccb150-523f-44f7-910a-a2713d2642c3"}]}